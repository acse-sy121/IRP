{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "==============================\\\n",
        "Student name: Shiqi Yin \\\n",
        "GitHub username: acse-sy121\\\n",
        "\\=============================="
      ],
      "metadata": {
        "id": "3qWeWcTWt_pT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxCJY0POfz7h"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import joblib\n",
        "import copy\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKeMrLeLk1yH",
        "outputId": "52203a94-8b55-42f1-95e3-7cdeb03a5c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data and scalers"
      ],
      "metadata": {
        "id": "EgVngsQSUtrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder, decoder = load_model('/content/drive/MyDrive/output_files/Models/with_sensor/autoencoder_5.h5').layers\n",
        "test_data = joblib.load('/content/drive/MyDrive/output_files/DATA/PCA_data/X_test_pca.pkl')\n",
        "pca_compress = joblib.load('/content/drive/MyDrive/output_files/DATA/PCA_data/pca_compress_to_92.pkl')\n",
        "\n",
        "test_ob_all_values = joblib.load('/content/drive/MyDrive/output_files/DATA/data_used_for_model/test_ob_all_values.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCFRi72-k10e",
        "outputId": "e1271c5c-af13-4640-fa36-4d7fc2d3b0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_CO2_test = joblib.load('/content/drive/MyDrive/output_files/scalers/node_data/scaler_CO2_test.pkl')\n",
        "scaler_x_test = joblib.load('/content/drive/MyDrive/output_files/scalers/node_data/scaler_x_test.pkl')\n",
        "scaler_y_test = joblib.load('/content/drive/MyDrive/output_files/scalers/node_data/scaler_y_test.pkl')\n",
        "scaler_z_test = joblib.load('/content/drive/MyDrive/output_files/scalers/node_data/scaler_z_test.pkl')\n",
        "scaler_Humidity_test = joblib.load('/content/drive/MyDrive/output_files/scalers/node_data/scaler_Humidity_test.pkl')\n",
        "scaler_Temperature_test = joblib.load('/content/drive/MyDrive/output_files/scalers/node_data/scaler_Temperature_test.pkl')\n",
        "scaler_Virus1_test = joblib.load('/content/drive/MyDrive/output_files/scalers/node_data/scaler_Virus1_test.pkl')\n",
        "scaler_01 = joblib.load('/content/drive/MyDrive/output_files/scalers/node_data/scaler_with_DA/scaler_minmax01.pkl')\n",
        "\n",
        "scaler_ob_CO2_test = joblib.load('/content/drive/MyDrive/output_files/scalers/sensor_data/scaler_ob_CO2_test.pkl')\n",
        "scaler_ob_xdata_test = joblib.load('/content/drive/MyDrive/output_files/scalers/sensor_data/scaler_ob_xdata_test.pkl')\n",
        "scaler_ob_ydata_test = joblib.load('/content/drive/MyDrive/output_files/scalers/sensor_data/scaler_ob_ydata_test.pkl')\n",
        "scaler_ob_zdata_test = joblib.load('/content/drive/MyDrive/output_files/scalers/sensor_data/scaler_ob_zdata_test.pkl')\n",
        "scaler_ob_Humidity_test = joblib.load('/content/drive/MyDrive/output_files/scalers/sensor_data/scaler_ob_Humidity_test.pkl')\n",
        "scaler_ob_Temperature_test = joblib.load('/content/drive/MyDrive/output_files/scalers/sensor_data/scaler_ob_Temperature_test.pkl')\n",
        "scaler_ob_Virus1_test = joblib.load('/content/drive/MyDrive/output_files/scalers/sensor_data/scaler_ob_Virus1_test.pkl')"
      ],
      "metadata": {
        "id": "AetX_LEBk15E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset pre-processing"
      ],
      "metadata": {
        "id": "FAbzrEgvUsnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_array_origin = scaler_01.transform(test_data)"
      ],
      "metadata": {
        "id": "bXX8PDDjk17W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_all = np.hstack((X_test_array_origin, test_ob_all_values))"
      ],
      "metadata": {
        "id": "4UeTtsjAk2AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_timestamps = 9\n",
        "batchsize = 128\n",
        "step = 1\n",
        "prediction_num = 136\n",
        "real_timestamp = 8\n",
        "x_axis = np.linspace(577, 577+real_timestamp+prediction_num-1, real_timestamp+prediction_num)\n",
        "x_input = np.linspace(577, 577+real_timestamp-1, real_timestamp)"
      ],
      "metadata": {
        "id": "h3B0MhpXotie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_axis"
      ],
      "metadata": {
        "id": "VjM07T5kf2yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simplify_dataset(X_train, input_timestamps, step):\n",
        "  X_train_simp = []\n",
        "  for i in range(len(X_train) - input_timestamps*step):\n",
        "    X_train_simp.append(X_train[i:i+input_timestamps*step:step])\n",
        "  return np.array(X_train_simp)"
      ],
      "metadata": {
        "id": "Dqr-oZMwk2FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_array = simplify_dataset(X_test_all, input_timestamps, step)"
      ],
      "metadata": {
        "id": "wNK3T5iQk2H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Data_reshape(input_array):\n",
        "  X_for_conv = (input_array.reshape((input_array.shape[0], input_timestamps, input_array.shape[2], 1))).astype('float32')\n",
        "  X_output = tf.data.Dataset.from_tensor_slices(X_for_conv)\n",
        "  X_output = X_output.shuffle(len(X_output))\n",
        "  X_output = X_output.batch(batchsize)\n",
        "  print(X_for_conv.shape)\n",
        "  return X_for_conv, X_output"
      ],
      "metadata": {
        "id": "2JCGnHIhk2Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_for_conv, X_test = Data_reshape(X_test_array)"
      ],
      "metadata": {
        "id": "d9TSTeHtk2M3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ec83df-3254-44da-ef9e-3724c2e5499a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(135, 9, 192, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of coeffs after PCA\n",
        "coeffs_num = X_test_array_origin.shape[1]\n",
        "# Number of sensors\n",
        "sensor_num = test_ob_all_values.shape[1]\n",
        "# Total coeffs input to the model\n",
        "total_coeffs = coeffs_num + sensor_num"
      ],
      "metadata": {
        "id": "PuMu3_vfk2Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start prediction"
      ],
      "metadata": {
        "id": "331FCcSIFyX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "def mse_loss(inp, outp):\n",
        "    \"\"\"\n",
        "    Calculate mean square error \n",
        "    between real value and prediction\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    inp : numpy.ndarray \n",
        "       real POD coefficients\n",
        "    ontp : numpy.ndarray\n",
        "       generated values of the POD coefficients\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "       mean squared loss between inp and outp\n",
        "    \"\"\"      \n",
        "    inp = tf.reshape(inp, [-1, total_coeffs])\n",
        "    outp = tf.reshape(outp, [-1, total_coeffs])\n",
        "    return mse(inp, outp)"
      ],
      "metadata": {
        "id": "qMJ3ZHCDk2SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_coding(initial_pred, real_coding):\n",
        "    \"\"\"\n",
        "    Generate predictions of m consecutive time levels\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    initial_pred : numpy.ndarray \n",
        "       initial guess of m consecutive time levels\n",
        "    real_coding : numpy.ndarray\n",
        "       real value of {0 - (m-2)} consecutive time levels\n",
        "       \n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray\n",
        "       predictions for m consecutive time levels\n",
        "    \"\"\"  \n",
        "    loss = []\n",
        "    # Do the iteration to get the result\n",
        "    for epoch in range(10):\n",
        "        encoder_output = encoder(initial_pred, training=False)\n",
        "        decoder_output = decoder(encoder_output, training=False)\n",
        "        loss.append(mse_loss(real_coding, decoder_output[:,:(input_timestamps - 1),:,:]).numpy())\n",
        "        # Replace initial guess with the output of decoder at time level n\n",
        "        initial_pred[:,(input_timestamps - 1):,:,:] = decoder_output[:,(input_timestamps - 1):,:,:]\n",
        "        \n",
        "    plt.plot(loss)\n",
        "    plt.grid()\n",
        "    plt.show \n",
        "        \n",
        "    return decoder_output, loss"
      ],
      "metadata": {
        "id": "io36ak_hk2U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start from time level 0\n",
        "n = 0\n",
        "real_value = X_test_for_conv[n].reshape(1,-1)\n",
        "# Set time level n-m to n-1 as the input\n",
        "real_value = real_value[:,:total_coeffs*(input_timestamps - 1)]\n",
        "real_value = real_value.reshape((1, input_timestamps - 1, X_test_for_conv.shape[2], 1))\n",
        "# Set value of time level n as same as that of time level n-1\n",
        "initial_pred = np.concatenate((real_value, real_value[:,-1:,:,:]), axis=1)\n",
        "\n",
        "# Predict a point forward in time (time level n)\n",
        "prediction_values,loss = predict_coding(initial_pred, real_value)\n",
        "# Update input and initial guess\n",
        "X_predict = list(prediction_values.numpy().reshape(-1,total_coeffs))\n",
        "# prediction of time level n\n",
        "gen_predict = prediction_values[:,(input_timestamps - 1):,:,:]\n",
        "# Add the predicted value to the real value\n",
        "real_value = np.concatenate((real_value[:,1:,:,:], gen_predict), axis=1)\n",
        "# Set value of time level n as same as that of time level n-1\n",
        "initial_pred = np.concatenate((real_value, real_value[:,-1:,:,:]), axis=1)\n",
        "\n",
        "# Predict prediction_num-1 points forward in time\n",
        "for i in tqdm.tqdm(range(0, prediction_num-1)):\n",
        "    prediction_values, loss = predict_coding(initial_pred, real_value)\n",
        "    # Update input values and initial guess\n",
        "    gen_predict = prediction_values[:,(input_timestamps - 1):,:,:].numpy()\n",
        "    X_predict.append(gen_predict.flatten())\n",
        "    real_value = np.concatenate((real_value[:,1:,:,:], gen_predict), axis=1)\n",
        "    initial_pred = np.concatenate((real_value, real_value[:,-1:,:,:]), axis=1)\n",
        "X_predict = np.array(X_predict)\n",
        "print(X_predict.shape)"
      ],
      "metadata": {
        "id": "47PDZNiUk2au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coeffs_pred = X_predict[:,:coeffs_num]\n",
        "sensors_pred = X_predict[:,coeffs_num:]"
      ],
      "metadata": {
        "id": "AqtRt9ZZk2dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process sensor results"
      ],
      "metadata": {
        "id": "DDV7j2gKj_TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_sensor_split = np.hsplit(sensors_pred, 7)"
      ],
      "metadata": {
        "id": "dtK0AP7rZUFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Undo (0,1) scaling\n",
        "pred_sensor_CO2 = scaler_ob_CO2_test.inverse_transform(pred_sensor_split[0])\n",
        "pred_sensor_xdata = scaler_ob_xdata_test.inverse_transform(pred_sensor_split[1])\n",
        "pred_sensor_ydata = scaler_ob_ydata_test.inverse_transform(pred_sensor_split[2])\n",
        "pred_sensor_zdata = scaler_ob_zdata_test.inverse_transform(pred_sensor_split[3])\n",
        "pred_sensor_Humidity = scaler_ob_Humidity_test.inverse_transform(pred_sensor_split[4])\n",
        "pred_sensor_Temperature = scaler_ob_Temperature_test.inverse_transform(pred_sensor_split[5])\n",
        "pred_sensor_Virus1 = scaler_ob_Virus1_test.inverse_transform(pred_sensor_split[6])"
      ],
      "metadata": {
        "id": "b2K843W8avD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ob_sensor_split = np.hsplit(test_ob_all_values, 7)"
      ],
      "metadata": {
        "id": "FduN_ZYTZUH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Undo (0,1) scaling\n",
        "ob_CO2_test = scaler_ob_CO2_test.inverse_transform(ob_sensor_split[0])\n",
        "ob_xdata_test = scaler_ob_xdata_test.inverse_transform(ob_sensor_split[1])\n",
        "ob_ydata_test = scaler_ob_ydata_test.inverse_transform(ob_sensor_split[2])\n",
        "ob_zdata_test = scaler_ob_zdata_test.inverse_transform(ob_sensor_split[3])\n",
        "ob_Humidity_test = scaler_ob_Humidity_test.inverse_transform(ob_sensor_split[4])\n",
        "ob_Temperature_test = scaler_ob_Temperature_test.inverse_transform(ob_sensor_split[5])\n",
        "ob_Virus1_test = scaler_ob_Virus1_test.inverse_transform(ob_sensor_split[6])"
      ],
      "metadata": {
        "id": "FWjcH8sVauY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the results of sensors"
      ],
      "metadata": {
        "id": "GbQP2uk_4WCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_sensor_CO2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWsKj8sL54rY",
        "outputId": "65268bd0-9a18-4da3-b35c-71918e42d686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(144, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_output_sensor(predict_data, CFD_data, filename):\n",
        "  fig, ax = plt.subplots(6, 3, figsize=[18,12])\n",
        "  fig.tight_layout(h_pad=3.5, w_pad=3.5)\n",
        "  for i in range(18):\n",
        "    ax.flatten()[i].plot(x_axis*5, predict_data[n:real_timestamp+prediction_num,i], '-')\n",
        "    ax.flatten()[i].plot(x_axis*5, CFD_data[n:n+(real_timestamp+prediction_num),i], '-')\n",
        "    ax.flatten()[i].plot(x_input*5, CFD_data[n:n+(real_timestamp),i], 'd')\n",
        "    ax.flatten()[i].legend(['Prediction', 'Fluidity', 'Input data'])\n",
        "    ax.flatten()[i].set_title('Sensor '+ str(i+1))\n",
        "    ax.flatten()[i].set_xlabel('Time (s)')\n",
        "    ax.flatten()[i].set_ylabel(filename)\n",
        "  path = '/content/drive/MyDrive/output_files/Figures/Figures_with_DA/Testing/sensors/'+str(filename)+'_prediction.png'\n",
        "  plt.savefig(path)"
      ],
      "metadata": {
        "id": "rYuJcc9Nk2gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output_sensor(pred_sensor_CO2, ob_CO2_test, 'CO2_ppm')"
      ],
      "metadata": {
        "id": "6Rkor14XcaLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output_sensor(pred_sensor_xdata, ob_xdata_test, 'Velocity_x')"
      ],
      "metadata": {
        "id": "xuJ5cqcqSvy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output_sensor(pred_sensor_ydata, ob_ydata_test, 'Velocity_y')"
      ],
      "metadata": {
        "id": "_K1Wso6LSv4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output_sensor(pred_sensor_zdata, ob_zdata_test, 'Velocity_z')"
      ],
      "metadata": {
        "id": "wHYsQxZJSv9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output_sensor(pred_sensor_Humidity, ob_Humidity_test, 'Humidity')"
      ],
      "metadata": {
        "id": "dP3boRNVcaOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output_sensor(pred_sensor_Temperature, ob_Temperature_test, 'Temperature')"
      ],
      "metadata": {
        "id": "qznGIuhYcaQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output_sensor(pred_sensor_Virus1, ob_Virus1_test, 'Virus1')"
      ],
      "metadata": {
        "id": "vtJbljt9caSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process nodes results"
      ],
      "metadata": {
        "id": "7fWU0JTMdmS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Undo (0,1) scaling\n",
        "X_inverse_01 = scaler_01.inverse_transform(coeffs_pred)\n",
        "# Undo PCA to obtain a scaled snapshot matrix\n",
        "X_inverse_PCA = pca_compress.inverse_transform(X_inverse_01)\n",
        "# Split to CO2 and three dimensions of velocity data\n",
        "X_predict_split = np.hsplit(X_inverse_PCA, 7)\n",
        "# Undo (−1,1) scaling\n",
        "pred_CO2 = scaler_CO2_test.inverse_transform(X_predict_split[0])\n",
        "pred_Velocity_x = scaler_x_test.inverse_transform(X_predict_split[1])\n",
        "pred_Velocity_y = scaler_y_test.inverse_transform(X_predict_split[2])\n",
        "pred_Velocity_z = scaler_z_test.inverse_transform(X_predict_split[3])\n",
        "pred_Humidity = scaler_Humidity_test.inverse_transform(X_predict_split[4])\n",
        "pred_Temperature = scaler_Temperature_test.inverse_transform(X_predict_split[5])\n",
        "pred_Virus1 = scaler_Virus1_test.inverse_transform(X_predict_split[6])"
      ],
      "metadata": {
        "id": "fXhHD509k2pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_CFD = pca_compress.inverse_transform(test_data)\n",
        "X_CFD_split = np.hsplit(X_CFD, 7)"
      ],
      "metadata": {
        "id": "QWCugEHyk2sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Undo (−1,1) scaling\n",
        "CFD_CO2 = scaler_CO2_test.inverse_transform(X_CFD_split[0])\n",
        "CFD_Velocity_x = scaler_x_test.inverse_transform(X_CFD_split[1])\n",
        "CFD_Velocity_y = scaler_y_test.inverse_transform(X_CFD_split[2])\n",
        "CFD_Velocity_z = scaler_z_test.inverse_transform(X_CFD_split[3])\n",
        "CFD_Humidity = scaler_Humidity_test.inverse_transform(X_CFD_split[4])\n",
        "CFD_Temperature = scaler_Temperature_test.inverse_transform(X_CFD_split[5])\n",
        "CFD_Virus1 = scaler_Virus1_test.inverse_transform(X_CFD_split[6])"
      ],
      "metadata": {
        "id": "cfoiV7gSk2vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the results of nodes"
      ],
      "metadata": {
        "id": "wZkN_W1OMrrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_output(predict_data, CFD_data, filename):\n",
        "  fig, ax = plt.subplots(3, 3, figsize=[16,10])\n",
        "  fig.tight_layout(h_pad=3.5, w_pad=3.5)\n",
        "  for i in range(9):\n",
        "    # max_scale, min_scale = calculate_scale(predict_data[:real_timestamp+prediction_num,i*200], \n",
        "    #                     CFD_data[n:n+(real_timestamp+prediction_num)*step:step,i*200])\n",
        "    # ax.flatten()[i].set_ylim([min_scale, max_scale])\n",
        "    ax.flatten()[i].plot(x_axis*5, predict_data[:real_timestamp+prediction_num,i*200], '-')\n",
        "    ax.flatten()[i].plot(x_axis*5, CFD_data[n:n+(real_timestamp+prediction_num)*step:step,i*200], '-')\n",
        "    ax.flatten()[i].plot(x_input*5, CFD_data[n:n+(real_timestamp)*step:step,i*200], 'd')\n",
        "    ax.flatten()[i].legend(['Prediction', 'Fluidity', 'Input data'])\n",
        "    ax.flatten()[i].set_title('Node '+ str(i*200+1))\n",
        "    ax.flatten()[i].set_xlabel('Time (s)')\n",
        "    ax.flatten()[i].set_ylabel(filename)\n",
        "  path = '/content/drive/MyDrive/output_files/Figures/Figures_with_DA/Testing/nodes/'+str(filename)+'_prediction.png'\n",
        "  plt.savefig(path)"
      ],
      "metadata": {
        "id": "flrZJo17k2yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output(pred_CO2, CFD_CO2, 'CO2_ppm')"
      ],
      "metadata": {
        "id": "11cQmnbYk20h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output(pred_Velocity_x, CFD_Velocity_x, 'Velocity_x')"
      ],
      "metadata": {
        "id": "KcCT5NuOKuHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output(pred_Velocity_y, CFD_Velocity_y, 'Velocity_y')"
      ],
      "metadata": {
        "id": "V-pbOp4zjlHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output(pred_Velocity_z, CFD_Velocity_z, 'Velocity_z')"
      ],
      "metadata": {
        "id": "Eafz1GYwrTz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output(pred_Humidity, CFD_Humidity, 'Humidity')"
      ],
      "metadata": {
        "id": "QeHZZY-wuHpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output(pred_Temperature, CFD_Temperature, 'Temperature')"
      ],
      "metadata": {
        "id": "9SKVxp69zpGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_output(pred_Virus1, CFD_Virus1, 'Virus1')"
      ],
      "metadata": {
        "id": "rNRklaO9zz8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output results to .vtu files"
      ],
      "metadata": {
        "id": "MrOYl3G2O3Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install vtk"
      ],
      "metadata": {
        "id": "yuCCvgE8WaxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive\")\n",
        "import vtktools\n",
        "from tools_io import get_nNodes_from_vtu, get_clean_vtu_file\n",
        "\n",
        "def write_example_to_vtu(i, exm_num):\n",
        "  # get clean vtu file\n",
        "  snapshot_data_location = '/content/drive/MyDrive/Cotrace_fixed_final/'\n",
        "  snapshot_file_base = 'Cotrace_fixed_'\n",
        "  filename = snapshot_data_location + snapshot_file_base + '582.vtu'\n",
        "  clean_vtu = get_clean_vtu_file(filename)\n",
        "\n",
        "  # write results to vtu\n",
        "  nNodes = get_nNodes_from_vtu(snapshot_data_location, snapshot_file_base)\n",
        "  CO2_ppm = np.zeros((nNodes,1))\n",
        "  velocity = np.zeros((nNodes,3))\n",
        "  humidity = np.zeros((nNodes,1))\n",
        "  temperature = np.zeros((nNodes,1))\n",
        "  virus1 = np.zeros((nNodes,1))\n",
        "\n",
        "  new_vtu = vtktools.vtu()\n",
        "  new_vtu.ugrid.DeepCopy(clean_vtu.ugrid)\n",
        "  new_vtu.filename = 'output_files/vtu_files/vtu_with_DA_not_applied/output_' + str(exm_num) + '.vtu'\n",
        "\n",
        "  CO2_ppm[:] = pred_CO2[i].reshape((192060, 1),order='F')\n",
        "  velocity[:,0] = pred_Velocity_x[i].reshape((192060),order='F')\n",
        "  velocity[:,1] = pred_Velocity_y[i].reshape((192060),order='F')\n",
        "  velocity[:,2] = pred_Velocity_z[i].reshape((192060),order='F')\n",
        "  humidity = pred_Humidity[i].reshape((192060, 1),order='F')\n",
        "  temperature = pred_Temperature[i].reshape((192060, 1),order='F')\n",
        "  virus1 = pred_Virus1[i].reshape((192060, 1),order='F')\n",
        "\n",
        "  new_vtu.AddField('Tracer', CO2_ppm)\n",
        "  new_vtu.AddField('Velocity', velocity)\n",
        "  new_vtu.AddField('Humidity', humidity)\n",
        "  new_vtu.AddField('Temperature', temperature)\n",
        "  new_vtu.AddField('Virus1', virus1)\n",
        "  new_vtu.Write()"
      ],
      "metadata": {
        "id": "M0YIx4n80GoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "start = 577\n",
        "end = 577+real_timestamp+prediction_num-1\n",
        "print(end)\n",
        "i = 0\n",
        "for filenum in tqdm.tqdm(range(start,end+1,1)):\n",
        "  # write_example_to_vtu(i, filenum)\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "-eI7XlAAWVTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1KUIJw72W0SL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}